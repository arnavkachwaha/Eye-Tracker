{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71191f06",
   "metadata": {},
   "source": [
    "# Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e978ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install coremltools\n",
    "!pip install pycocotools\n",
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install pytorch-lightning\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# conda install -c conda-forge gcc libstdcxx-ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013bec9",
   "metadata": {},
   "source": [
    "# Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import coremltools as ct\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image,ImageDraw\n",
    "import torch.nn.functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning import Trainer\n",
    "from torchmetrics import JaccardIndex\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from scipy.ndimage import label, find_objects, distance_transform_edt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0fb892",
   "metadata": {},
   "source": [
    "# Setting SLURM variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3022677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM Environment Variables:\n",
      "SLURM_JOB_ID: 6072005\n",
      "SLURM_NTASKS: Not Set\n",
      "SLURM_NTASKS_PER_NODE: 8\n",
      "SLURM_JOB_NODELIST: s1cmp004\n",
      "SLURM_JOB_NAME: sys/dashboard/sys/jupyternotebook\n"
     ]
    }
   ],
   "source": [
    "os.environ['SLURM_NTASKS_PER_NODE'] = '8'\n",
    "os.environ.pop('SLURM_NTASKS', None)\n",
    "\n",
    "# Print SLURM environment variables to verify\n",
    "print(\"SLURM Environment Variables:\")\n",
    "print(\"SLURM_JOB_ID:\", os.environ.get('SLURM_JOB_ID', 'Not Set'))\n",
    "print(\"SLURM_NTASKS:\", os.environ.get('SLURM_NTASKS', 'Not Set'))\n",
    "print(\"SLURM_NTASKS_PER_NODE:\", os.environ.get('SLURM_NTASKS_PER_NODE', 'Not Set'))\n",
    "print(\"SLURM_JOB_NODELIST:\", os.environ.get('SLURM_JOB_NODELIST', 'Not Set'))\n",
    "print(\"SLURM_JOB_NAME:\", os.environ.get('SLURM_JOB_NAME', 'Not Set'))\n",
    "\n",
    "# Set float32 matmul precision for Tensor Cores\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f95670",
   "metadata": {},
   "source": [
    "# Defining file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11fcbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image paths\n",
    "test_dir = './images/merged_test'\n",
    "train_dir = './images/merged_train'\n",
    "val_dir = './images/merged_val'\n",
    "\n",
    "#Annotation Paths\n",
    "test_ann_file = './annotations/test_merged_7_24_24.json'\n",
    "train_ann_file = './annotations/train_merged_7_24_24.json'\n",
    "val_ann_file = './annotations/merged_val_07_24_24.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90e6ee",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2ff0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_normalization(img_dir, annotation_file):\n",
    "    coco = COCO(annotation_file)\n",
    "    img_ids = list(coco.imgs.keys())\n",
    "    means = []\n",
    "    stds = []\n",
    "\n",
    "    for img_id in img_ids:\n",
    "        img_info = coco.imgs[img_id]\n",
    "        path = os.path.join(img_dir, img_info['file_name'])\n",
    "        image = Image.open(path).convert('RGB')\n",
    "\n",
    "        # Convert image to tensor without normalization\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        image_tensor = to_tensor(image)\n",
    "        \n",
    "        means.append(image_tensor.mean(dim=(1, 2)))\n",
    "        stds.append(image_tensor.std(dim=(1, 2)))\n",
    "\n",
    "    mean = torch.stack(means).mean(dim=0)\n",
    "    std = torch.stack(stds).mean(dim=0)\n",
    "\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6fbfd",
   "metadata": {},
   "source": [
    "# Defining COCO loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8162d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader for COCO formatted data\n",
    "def get_coco_loader(img_dir, annotation_file, transform, augment=False, num_augmentations=1):\n",
    "    coco = COCO(annotation_file)\n",
    "    img_ids = list(coco.imgs.keys())\n",
    "    def loader():\n",
    "        for img_id in img_ids:\n",
    "            img_info = coco.imgs[img_id]\n",
    "            path = os.path.join(img_dir, img_info['file_name'])\n",
    "            image = Image.open(path).convert('RGB')\n",
    "            image_tensor = transform(image)\n",
    "            ann_ids = coco.getAnnIds(imgIds=[img_id])\n",
    "            anns = coco.loadAnns(ann_ids)\n",
    "            mask = np.zeros((img_info['height'], img_info['width']), dtype=np.int64)\n",
    "            for ann in anns:\n",
    "                mask = np.maximum(mask, coco.annToMask(ann) * ann['category_id'])\n",
    "            mask_tensor = torch.tensor(mask)\n",
    "            yield image_tensor, mask_tensor\n",
    "            \n",
    "            if augment:\n",
    "                for _ in range(num_augmentations):\n",
    "                    aug_image, aug_mask = apply_transform(image, mask)\n",
    "                    aug_image_tensor = transform(aug_image)\n",
    "                    aug_mask_tensor = torch.tensor(aug_mask, dtype=torch.int64)\n",
    "                    yield aug_image_tensor, aug_mask_tensor\n",
    "    return loader\n",
    "\n",
    "def get_position_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),  \n",
    "        transforms.RandomRotation(15),      \n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def get_color_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Randomly changes the brightness, contrast, saturation, and hue\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def apply_transform(image, mask):\n",
    "    # Ensure the mask is in uint8 format then convert to PIL Image\n",
    "    mask = mask.astype(np.uint8)  \n",
    "    mask = Image.fromarray(mask) \n",
    "\n",
    "    # seed for random transformations\n",
    "    seed = torch.randint(0, 2**32, (1,)).item()\n",
    "\n",
    "    # Apply the same position transformation to both image and mask\n",
    "    torch.manual_seed(seed)\n",
    "    image = get_position_transform()(image)\n",
    "    torch.manual_seed(seed)\n",
    "    mask = get_position_transform()(mask)\n",
    "\n",
    "    # apply color transformation to image\n",
    "    image = get_color_transform()(image)\n",
    "\n",
    "    # Convert mask back to numpy array\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "# Custom DataLoader for COCO data\n",
    "class COCOLoader(IterableDataset):\n",
    "    def __init__(self, img_dir, annotation_file, transform, augment=False, num_augmentations=1):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.annotation_file = annotation_file\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.num_augmentations = num_augmentations\n",
    "\n",
    "    def __iter__(self):\n",
    "        return get_coco_loader(self.img_dir, self.annotation_file, self.transform, self.augment, self.num_augmentations)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8f4d3",
   "metadata": {},
   "source": [
    "# Defining Test-Train-Val loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf104f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data tranforms\n",
    "test_transform = dynamic_normalization(test_dir, test_ann_file)\n",
    "train_transform = dynamic_normalization(train_dir, train_ann_file)\n",
    "val_transform = dynamic_normalization(val_dir, val_ann_file)\n",
    "\n",
    "\n",
    "# # Setup data loaders\n",
    "test_loader = DataLoader(COCOLoader(test_dir, test_ann_file, test_transform, False), batch_size=32, num_workers=4)\n",
    "train_loader = DataLoader(COCOLoader(train_dir, train_ann_file, train_transform), batch_size=32, num_workers=4)\n",
    "val_loader = DataLoader(COCOLoader(val_dir, val_ann_file, val_transform), batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72dc60",
   "metadata": {},
   "source": [
    "# Visualize Images with annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, masks in val_loader:\n",
    "    # Convert tensors to numpy arrays\n",
    "    images_np = images.permute(0, 2, 3, 1).numpy()  # Convert from (N, C, H, W) to (N, H, W, C)\n",
    "    masks_np = masks.numpy()\n",
    "\n",
    "    # Plot images and masks\n",
    "    for i in range(len(images)):\n",
    "        image_np = images_np[i]\n",
    "        mask_np = masks_np[i]\n",
    "\n",
    "        # Overlay mask on image\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(image_np)\n",
    "        plt.imshow(mask_np, alpha=0.5, cmap='jet', interpolation='nearest')  # Overlay mask on image\n",
    "        plt.title('Image with Pupil Highlighted')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dcc620",
   "metadata": {},
   "source": [
    "# Function to calculate Mean-Iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9276d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target, num_classes):\n",
    "    iou = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).sum().item()\n",
    "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
    "        if union == 0:\n",
    "            iou.append(float('nan'))  # If no ground truth pixels exist in this class\n",
    "        else:\n",
    "            iou.append(intersection / union)\n",
    "    return torch.tensor(iou).mean()  # Return mean IoU over classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9151a",
   "metadata": {},
   "source": [
    "# Segformer Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce8d98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module for Segformer\n",
    "class SegformerModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            'nvidia/segformer-b0-finetuned-ade-512-512',\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.test_losses = []\n",
    "        self.test_ious = []\n",
    "        self.metrics = torchmetrics.JaccardIndex(task='multiclass', num_classes=self.model.config.num_labels, average='macro')\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        return self.model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        # Upsample logits to match the size of labels\n",
    "        logits = outputs.logits\n",
    "        logits = F.interpolate(logits, size=labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.metrics.update(preds, labels)\n",
    "        self.log('train_iou', self.metrics, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images, labels=labels)\n",
    "        val_loss = outputs.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "        # Upsample logits to match the size of labels\n",
    "        logits = outputs.logits\n",
    "        logits = F.interpolate(logits, size=labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.metrics.update(preds, labels)\n",
    "        self.log('val_iou', self.metrics, on_step=False, on_epoch=True)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "        # Upsample logits to match the size of labels\n",
    "        logits = outputs.logits\n",
    "        logits = F.interpolate(logits, size=labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.metrics.update(preds, labels)\n",
    "        self.log('test_iou', self.metrics, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.test_losses.append(loss.detach())\n",
    "        return {'test_loss': loss}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.test_losses).mean()\n",
    "        avg_iou = self.metrics.compute()  # Compute the final IoU score\n",
    "        self.log('avg_test_loss', avg_loss)\n",
    "        self.log('avg_iou_score', avg_iou)\n",
    "        self.test_losses.clear()\n",
    "        self.test_ious.clear()\n",
    "        self.metrics.reset()  # Reset metric states for the next epoch\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.0001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check If cuda is available\n",
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319528f",
   "metadata": {},
   "source": [
    "# Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9edeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and checkpoint\n",
    "num_classes = 2 \n",
    "segformer = SegformerModule(num_classes)\n",
    "checkpoint_callback = ModelCheckpoint(monitor = 'val_loss', mode = 'min', dirpath = './checkpoints')\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"segformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86d6d4",
   "metadata": {},
   "source": [
    "# Defining trainer attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(max_epochs= 128,\n",
    "                  accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "                  devices = 1, \n",
    "                  callbacks=[checkpoint_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7d060",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f16e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(segformer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a08b67",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15195f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)Done (t=0.02s)\n",
      "creating index...\n",
      "\n",
      "creating index...index created!\n",
      "loading annotations into memory...\n",
      "\n",
      "index created!\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb01566b99d9463cbb2a362c6799d7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      avg_iou_score         0.8706876039505005\n",
      "      avg_test_loss        0.026751693338155746\n",
      "        test_iou            0.8706876039505005\n",
      "        test_loss          0.030302807688713074\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.030302807688713074,\n",
       "  'test_iou': 0.8706876039505005,\n",
       "  'avg_test_loss': 0.026751693338155746,\n",
       "  'avg_iou_score': 0.8706876039505005}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "trainer.test(model = segformer, dataloaders = test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339ee91",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af349014",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './checkpoints/segformer_model.pth'\n",
    "torch.save(segformer.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fbe0c",
   "metadata": {},
   "source": [
    "# Loading model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c293c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegformerModule(\n",
       "  (model): SegformerForSemanticSegmentation(\n",
       "    (segformer): SegformerModel(\n",
       "      (encoder): SegformerEncoder(\n",
       "        (patch_embeddings): ModuleList(\n",
       "          (0): SegformerOverlapPatchEmbeddings(\n",
       "            (proj): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "            (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SegformerOverlapPatchEmbeddings(\n",
       "            (proj): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SegformerOverlapPatchEmbeddings(\n",
       "            (proj): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SegformerOverlapPatchEmbeddings(\n",
       "            (proj): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (block): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "                  (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "                  (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
       "                  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "                  (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "                  (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
       "                  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.014285714365541935)\n",
       "              (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "                  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.02857142873108387)\n",
       "              (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "                  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.04285714402794838)\n",
       "              (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=160, out_features=160, bias=True)\n",
       "                  (key): Linear(in_features=160, out_features=160, bias=True)\n",
       "                  (value): Linear(in_features=160, out_features=160, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
       "                  (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=160, out_features=160, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.05714285746216774)\n",
       "              (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=160, out_features=160, bias=True)\n",
       "                  (key): Linear(in_features=160, out_features=160, bias=True)\n",
       "                  (value): Linear(in_features=160, out_features=160, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
       "                  (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=160, out_features=160, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.0714285746216774)\n",
       "              (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.08571428805589676)\n",
       "              (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SegformerLayer(\n",
       "              (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SegformerAttention(\n",
       "                (self): SegformerEfficientSelfAttention(\n",
       "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SegformerSelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SegformerDropPath(p=0.10000000149011612)\n",
       "              (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): SegformerMixFFN(\n",
       "                (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (dwconv): SegformerDWConv(\n",
       "                  (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                )\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): ModuleList(\n",
       "          (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decode_head): SegformerDecodeHead(\n",
       "      (linear_c): ModuleList(\n",
       "        (0): SegformerMLP(\n",
       "          (proj): Linear(in_features=32, out_features=256, bias=True)\n",
       "        )\n",
       "        (1): SegformerMLP(\n",
       "          (proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "        )\n",
       "        (2): SegformerMLP(\n",
       "          (proj): Linear(in_features=160, out_features=256, bias=True)\n",
       "        )\n",
       "        (3): SegformerMLP(\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (linear_fuse): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (metrics): MulticlassJaccardIndex()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `checkpoint_callback.best_model_path` contains the path to the best model checkpoint\n",
    "model_path = checkpoint_callback.best_model_path\n",
    "segformer = SegformerModule.load_from_checkpoint(model_path, num_classes=2)\n",
    "segformer.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "segformer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9593b",
   "metadata": {},
   "source": [
    "# Visualizing model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to de-normalize the image\n",
    "def denormalize(image_tensor, mean, std):\n",
    "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format\n",
    "    mean = np.array(mean)\n",
    "    std = np.array(std)\n",
    "    image_np = std * image_np + mean  # De-normalize\n",
    "    image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)  # Convert to uint8\n",
    "    return image_np\n",
    "\n",
    "\n",
    "# def denormalize(image_tensor):\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     image_np = image_tensor.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format\n",
    "#     image_np = std * image_np + mean  # De-normalize\n",
    "#     image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)  # Convert to uint8\n",
    "#     return image_np\n",
    "\n",
    "color_map = {\n",
    "    0: (0, 0, 0),   # Background\n",
    "    1: (255, 0, 0), # Original mask\n",
    "    2: (0, 255, 0), # Predicted mask\n",
    "}\n",
    "\n",
    "# Define mean and std for normalization\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Ensure your existing model instance is in evaluation mode\n",
    "segformer.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "segformer.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Function to overlay mask\n",
    "def overlay_mask(image_np, mask_np, color):\n",
    "    vis_shape = image_np.shape\n",
    "    overlay = image_np.copy()\n",
    "    for i in range(3):\n",
    "        overlay[:, :, i] = np.where(mask_np == 1, color[i], overlay[:, :, i])\n",
    "    return Image.blend(Image.fromarray(image_np), Image.fromarray(overlay), alpha=0.5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = segformer(images)\n",
    "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "\n",
    "        upsampled_logits = torch.nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1).cpu().numpy()\n",
    "        masks = masks.cpu().numpy()\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            image_np = denormalize(images[i], mean, std)\n",
    "            original_mask_np = masks[i]\n",
    "            pred_mask_np = predicted[i]\n",
    "\n",
    "            original_overlay = overlay_mask(image_np, original_mask_np, color_map[1])\n",
    "            predicted_overlay = overlay_mask(image_np, pred_mask_np, color_map[1])\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(original_overlay)\n",
    "            plt.title('Image with Original Mask')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(predicted_overlay)\n",
    "            plt.title('Image with Predicted Mask')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcbf591",
   "metadata": {},
   "source": [
    "# Creating CoreMl object of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84630a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the existing model instance is in evaluation mode\n",
    "segformer.eval()\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "segformer.to(device)\n",
    "\n",
    "# Extract the underlying HuggingFace model from the LightningModule\n",
    "underlying_model = segformer.model\n",
    "\n",
    "# Dummy input to match the expected input size (batch_size, channels, height, width)\n",
    "dummy_input = torch.randn(32, 3, 640, 480).to(device)\n",
    "\n",
    "# Trace the model\n",
    "traced_model = torch.jit.trace(underlying_model, dummy_input)\n",
    "\n",
    "# Convert the traced model to Core ML\n",
    "coreml_model = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=[ct.ImageType(name=\"input\", shape=dummy_input.shape, scale=1/255.0, bias=[-0.485, -0.456, -0.406])]\n",
    ")\n",
    "\n",
    "# Save the Core ML model\n",
    "coreml_model.save('cyclops_coreml.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3e40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insight2024",
   "language": "python",
   "name": "insight2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
